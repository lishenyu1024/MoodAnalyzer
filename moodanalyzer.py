# -*- coding: utf-8 -*-
"""MoodAnalyzeripynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G2-uVseGgviFp8onk85cU4qRD5S9I1d1
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd


data = pd.read_csv("synthetic_social_media_data.csv")

print(data.head())



""":) begin!!"""

#1. Check the structure and missing values ​​of the data
#First, check the data in more detail to understand the columns and data types of the data and check whether there are any missing values.#

# View basic information of the data, such as data type and missing values
print(data.info())

# View statistical information of the data (mean, standard deviation, etc. of numerical data)
print(data.describe())

# Check if there are missing values
print(data.isnull().sum())

data = data.dropna()

#2. Data type conversion
#Next, need to check whether the data type of each column is suitable for machine learning.


#From the information, the Post Content column is text data,
#which needs to be converted into numerical features before it can be input into the machine learning model.
#use TF-IDF or bag-of-words model for conversion.#

# version1:TfidfVectorizer accuracy 31%
# from sklearn.feature_extraction.text import TfidfVectorizer

# # Extract text data
# texts = data['Post Content']

# # Initialize TF-IDF Vectorizer
# tfidf_vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2)) # Limit the maximum number of features to 10000
# X_text = tfidf_vectorizer.fit_transform(texts).toarray()

# print(X_text.shape)

# #version2:CountVectorizer
from sklearn.feature_extraction.text import CountVectorizer

# Use CountVectorizer instead of TfidfVectorizer
count_vectorizer = CountVectorizer(max_features=10000, ngram_range=(1, 2))  # Capture both unigrams and bigrams
X_text = count_vectorizer.fit_transform(texts).toarray()


print(X_text.shape)

# For categorical data like Post Type and Language, use One-Hot Encoding or Label Encoding to convert to numeric values.

# from sklearn.preprocessing import OneHotEncoder

# from sklearn.compose import ColumnTransformer

# # Assume 'Post Type' and 'Language' are categorical data
# categorical_features = ['Post Type', 'Language']

# # OneHot Encode categorical data
# preprocessor = ColumnTransformer(
# transformers=[
# ('cat', OneHotEncoder(), categorical_features)
# ])

# X_cat = preprocessor.fit_transform(data)

# print(X_cat.shape)
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
import numpy as np

# Step 1: Text feature processing (e.g. using TF-IDF)
from sklearn.feature_extraction.text import TfidfVectorizer
tfidf_vectorizer = TfidfVectorizer(max_features=5000)
X_text = tfidf_vectorizer.fit_transform(data['Post Content']).toarray()

# Step 2: Numerical feature standardization
numerical_features = ['Number of Likes', 'Number of Shares', 'User Follower Count']
scaler = StandardScaler()
X_num = scaler.fit_transform(data[numerical_features])

# Step 3: One-Hot encoding of categorical features
categorical_features = ['Post Type', 'Language']
preprocessor = ColumnTransformer(
transformers=[
('cat', OneHotEncoder(), categorical_features)
]
)
X_cat = preprocessor.fit_transform(data)

# Step 4: Merge all features
X = np.hstack([X_text, X_cat.toarray(), X_num]) # Merge text, categorical, and numerical features
print(X.shape)

# 3. Processing numerical data
#For numerical data such as Number of Likes and Number of Shares, perform Normalization or Standardization.

from sklearn.preprocessing import StandardScaler

# Select numerical feature columns
numerical_features = ['Number of Likes', 'Number of Shares', 'Number of Comments', 'User Follower Count']

# Standardize numerical data
scaler = StandardScaler()
X_num = scaler.fit_transform(data[numerical_features])

print(X_num.shape)

# 4. Merge all features
# Merge the processed text features (X_text), category features (X_cat), and numerical features (X_num) into an overall feature matrix X

import numpy as np

# Merge text features, category features, and numerical features
X = np.hstack([X_text, X_cat.toarray(), X_num])

# View the merged data
print(X.shape)

# 5. Processing label data
#For the target variable Sentiment Label, you need to convert it into a digital form. Generally speaking, the labels for sentiment analysis are usually categorical data (e.g. Positive, Negative, Neutral). Try using Label Encoding.

from sklearn.preprocessing import LabelEncoder

# Encode the sentiment label
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(data['Sentiment Label'])

# View the label
print(np.unique(y))

# 6. Split the dataset
#Split the dataset into training and test sets. This time I will use 80% training set and 20% test set.
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# View the shape of the data after splitting
print(X_train.shape, X_test.shape)

import numpy as np
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.layers import Dropout, Dense, Input
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.optimizers.schedules import ExponentialDecay
from imblearn.over_sampling import SMOTE
from collections import Counter
# Assume that the original label of y is 0 (Negative), 1 (Neutral), 2 (Positive)
# 1. Merge labels into two categories
# Merge 0 and 1 into class 0, and keep 2 as class 1
y_binary = np.where(y == 2, 1, 0) # 0 means Negative + Neutral, 1 means Positive

# 2. Merge all features
X = np.hstack([X_text, X_cat.toarray(), X_num])
print(f"Feature shape: {X.shape}")

# 3. Divide the dataset
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)

# 4. Apply SMOTE Oversampling
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)
print(f"Resampled class distribution: {Counter(y_train_resampled)}")

# 5. Calculate class weights
class_weights = compute_class_weight(
class_weight='balanced',
classes=np.unique(y_train_resampled),
y=y_train_resampled
)
class_weight_dict = dict(enumerate(class_weights))
print(f"Class weights: {class_weight_dict}")

# 6. Build model
model = Sequential()

# Input layer
model.add(Input(shape=(X_train.shape[1],)))

# Hidden layer
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.6))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.6))

# Output layer (1 neuron + sigmoid activation function)
model.add(Dense(1, activation='sigmoid'))

# 7. Compile model
initial_lr = 0.005
lr_schedule = ExponentialDecay(
initial_learning_rate=initial_lr,
decay_steps=100000,
decay_rate=0.99,
staircase=True
)
model.compile(optimizer=Adam(learning_rate=lr_schedule), loss='binary_crossentropy', metrics=['accuracy'])

# 8. Early stopping mechanism
early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)

# 9. Train the model
history = model.fit(
X_train_resampled, y_train_resampled,
epochs=20,
batch_size=32,
validation_data=(X_test, y_test),
callbacks=[early_stopping],
class_weight=class_weight_dict
)

# 10. Evaluate the model
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_accuracy}")

# 11. Predictions and confusion matrix
y_pred_prob = model.predict(X_test)
y_pred = (y_pred_prob > 0.5).astype(int)

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative+Neutral', 'Positive'], yticklabels=['Negative+Neutral', 'Positive'])
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show() # Classification report print(classification_report(y_test, y_pred, target_names=['Negative+Neutral', 'Positive']))

#8. Train the model
#Now, the exciting part is to train the model using the training data：）
# Train the model
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))

# View the accuracy and loss during training
import matplotlib.pyplot as plt

plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

#9. Evaluate the model
#After training, evaluate the performance of the model.
#Evaluate the model
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_accuracy}")